在开始建立模型之前，执行特征选定有助于：
#降低数据的拟合度：较少的冗余数据，会使算法得出结论的机会更大；
#提高算法精度：较少的误导数据，能够提高算法的准确度；
#减少训练时间：越少的数据，训练模型所需要的时间越少。

#单变量特征选定#

卡方检验是检验定性自变量对定性因变量的相关性的方法。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望值的差距，
构建统计量。卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，卡方值越大，越不符合

#递归特征消除#

递归特征消除（RFE）使用一个基模型来进行多轮训练，每轮训练后消除若干权值系数的特征，再基于新的特征集进行下一轮训练。通过每一个基模型的精度，找到对最终的
预测结果影响最大的数据特征。

#主要成分分析#

主要成分分析（PCA）是使用线性代数来转换压缩数据，通常被称作数据降维。PCA是为了让映射后的样本具有最大的发散性；而线性判别分析（LDA）是为了让映射后的样
本有最好的分类性能。PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。在聚类算法中，通常会利用PCA对数据进行降维处理。

#特征重要性#

袋装决策树算法、随机森林算法和极端随机树算法都可以用来计算数据特征的重要性，这三个算法都是集成算法中的袋装算法。
